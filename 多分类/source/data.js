
// 定义鸢尾花的3种类型。
const IRIS_CLASSES = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'];
const IRIS_NUM_CLASSES = IRIS_CLASSES.length;

// Iris flowers data. Source: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
// 鸢尾花分类的原始数据，格式为二元数组，每个子数组为一个鸢尾花样本。子数组中的前4个元素为
// 每个鸢尾花样本的特征：花瓣（Petal）长度、花瓣宽度、萼片（Sepal）长度、萼片宽度，第5个元素为鸢尾花的类型，
// 值为0、1、2，即iris-setosa、iris-versicolor和iris-virginica这3种类型。
const IRIS_DATA = [
    [5.1, 3.5, 1.4, 0.2, 0], [4.9, 3.0, 1.4, 0.2, 0], [4.7, 3.2, 1.3, 0.2, 0],
    [4.6, 3.1, 1.5, 0.2, 0], [5.0, 3.6, 1.4, 0.2, 0], [5.4, 3.9, 1.7, 0.4, 0],
    [4.6, 3.4, 1.4, 0.3, 0], [5.0, 3.4, 1.5, 0.2, 0], [4.4, 2.9, 1.4, 0.2, 0],
    [4.9, 3.1, 1.5, 0.1, 0], [5.4, 3.7, 1.5, 0.2, 0], [4.8, 3.4, 1.6, 0.2, 0],
    [4.8, 3.0, 1.4, 0.1, 0], [4.3, 3.0, 1.1, 0.1, 0], [5.8, 4.0, 1.2, 0.2, 0],
    [5.7, 4.4, 1.5, 0.4, 0], [5.4, 3.9, 1.3, 0.4, 0], [5.1, 3.5, 1.4, 0.3, 0],
    [5.7, 3.8, 1.7, 0.3, 0], [5.1, 3.8, 1.5, 0.3, 0], [5.4, 3.4, 1.7, 0.2, 0],
    [5.1, 3.7, 1.5, 0.4, 0], [4.6, 3.6, 1.0, 0.2, 0], [5.1, 3.3, 1.7, 0.5, 0],
    [4.8, 3.4, 1.9, 0.2, 0], [5.0, 3.0, 1.6, 0.2, 0], [5.0, 3.4, 1.6, 0.4, 0],
    [5.2, 3.5, 1.5, 0.2, 0], [5.2, 3.4, 1.4, 0.2, 0], [4.7, 3.2, 1.6, 0.2, 0],
    [4.8, 3.1, 1.6, 0.2, 0], [5.4, 3.4, 1.5, 0.4, 0], [5.2, 4.1, 1.5, 0.1, 0],
    [5.5, 4.2, 1.4, 0.2, 0], [4.9, 3.1, 1.5, 0.1, 0], [5.0, 3.2, 1.2, 0.2, 0],
    [5.5, 3.5, 1.3, 0.2, 0], [4.9, 3.1, 1.5, 0.1, 0], [4.4, 3.0, 1.3, 0.2, 0],
    [5.1, 3.4, 1.5, 0.2, 0], [5.0, 3.5, 1.3, 0.3, 0], [4.5, 2.3, 1.3, 0.3, 0],
    [4.4, 3.2, 1.3, 0.2, 0], [5.0, 3.5, 1.6, 0.6, 0], [5.1, 3.8, 1.9, 0.4, 0],
    [4.8, 3.0, 1.4, 0.3, 0], [5.1, 3.8, 1.6, 0.2, 0], [4.6, 3.2, 1.4, 0.2, 0],
    [5.3, 3.7, 1.5, 0.2, 0], [5.0, 3.3, 1.4, 0.2, 0], [7.0, 3.2, 4.7, 1.4, 1],
    [6.4, 3.2, 4.5, 1.5, 1], [6.9, 3.1, 4.9, 1.5, 1], [5.5, 2.3, 4.0, 1.3, 1],
    [6.5, 2.8, 4.6, 1.5, 1], [5.7, 2.8, 4.5, 1.3, 1], [6.3, 3.3, 4.7, 1.6, 1],
    [4.9, 2.4, 3.3, 1.0, 1], [6.6, 2.9, 4.6, 1.3, 1], [5.2, 2.7, 3.9, 1.4, 1],
    [5.0, 2.0, 3.5, 1.0, 1], [5.9, 3.0, 4.2, 1.5, 1], [6.0, 2.2, 4.0, 1.0, 1],
    [6.1, 2.9, 4.7, 1.4, 1], [5.6, 2.9, 3.6, 1.3, 1], [6.7, 3.1, 4.4, 1.4, 1],
    [5.6, 3.0, 4.5, 1.5, 1], [5.8, 2.7, 4.1, 1.0, 1], [6.2, 2.2, 4.5, 1.5, 1],
    [5.6, 2.5, 3.9, 1.1, 1], [5.9, 3.2, 4.8, 1.8, 1], [6.1, 2.8, 4.0, 1.3, 1],
    [6.3, 2.5, 4.9, 1.5, 1], [6.1, 2.8, 4.7, 1.2, 1], [6.4, 2.9, 4.3, 1.3, 1],
    [6.6, 3.0, 4.4, 1.4, 1], [6.8, 2.8, 4.8, 1.4, 1], [6.7, 3.0, 5.0, 1.7, 1],
    [6.0, 2.9, 4.5, 1.5, 1], [5.7, 2.6, 3.5, 1.0, 1], [5.5, 2.4, 3.8, 1.1, 1],
    [5.5, 2.4, 3.7, 1.0, 1], [5.8, 2.7, 3.9, 1.2, 1], [6.0, 2.7, 5.1, 1.6, 1],
    [5.4, 3.0, 4.5, 1.5, 1], [6.0, 3.4, 4.5, 1.6, 1], [6.7, 3.1, 4.7, 1.5, 1],
    [6.3, 2.3, 4.4, 1.3, 1], [5.6, 3.0, 4.1, 1.3, 1], [5.5, 2.5, 4.0, 1.3, 1],
    [5.5, 2.6, 4.4, 1.2, 1], [6.1, 3.0, 4.6, 1.4, 1], [5.8, 2.6, 4.0, 1.2, 1],
    [5.0, 2.3, 3.3, 1.0, 1], [5.6, 2.7, 4.2, 1.3, 1], [5.7, 3.0, 4.2, 1.2, 1],
    [5.7, 2.9, 4.2, 1.3, 1], [6.2, 2.9, 4.3, 1.3, 1], [5.1, 2.5, 3.0, 1.1, 1],
    [5.7, 2.8, 4.1, 1.3, 1], [6.3, 3.3, 6.0, 2.5, 2], [5.8, 2.7, 5.1, 1.9, 2],
    [7.1, 3.0, 5.9, 2.1, 2], [6.3, 2.9, 5.6, 1.8, 2], [6.5, 3.0, 5.8, 2.2, 2],
    [7.6, 3.0, 6.6, 2.1, 2], [4.9, 2.5, 4.5, 1.7, 2], [7.3, 2.9, 6.3, 1.8, 2],
    [6.7, 2.5, 5.8, 1.8, 2], [7.2, 3.6, 6.1, 2.5, 2], [6.5, 3.2, 5.1, 2.0, 2],
    [6.4, 2.7, 5.3, 1.9, 2], [6.8, 3.0, 5.5, 2.1, 2], [5.7, 2.5, 5.0, 2.0, 2],
    [5.8, 2.8, 5.1, 2.4, 2], [6.4, 3.2, 5.3, 2.3, 2], [6.5, 3.0, 5.5, 1.8, 2],
    [7.7, 3.8, 6.7, 2.2, 2], [7.7, 2.6, 6.9, 2.3, 2], [6.0, 2.2, 5.0, 1.5, 2],
    [6.9, 3.2, 5.7, 2.3, 2], [5.6, 2.8, 4.9, 2.0, 2], [7.7, 2.8, 6.7, 2.0, 2],
    [6.3, 2.7, 4.9, 1.8, 2], [6.7, 3.3, 5.7, 2.1, 2], [7.2, 3.2, 6.0, 1.8, 2],
    [6.2, 2.8, 4.8, 1.8, 2], [6.1, 3.0, 4.9, 1.8, 2], [6.4, 2.8, 5.6, 2.1, 2],
    [7.2, 3.0, 5.8, 1.6, 2], [7.4, 2.8, 6.1, 1.9, 2], [7.9, 3.8, 6.4, 2.0, 2],
    [6.4, 2.8, 5.6, 2.2, 2], [6.3, 2.8, 5.1, 1.5, 2], [6.1, 2.6, 5.6, 1.4, 2],
    [7.7, 3.0, 6.1, 2.3, 2], [6.3, 3.4, 5.6, 2.4, 2], [6.4, 3.1, 5.5, 1.8, 2],
    [6.0, 3.0, 4.8, 1.8, 2], [6.9, 3.1, 5.4, 2.1, 2], [6.7, 3.1, 5.6, 2.4, 2],
    [6.9, 3.1, 5.1, 2.3, 2], [5.8, 2.7, 5.1, 1.9, 2], [6.8, 3.2, 5.9, 2.3, 2],
    [6.7, 3.3, 5.7, 2.5, 2], [6.7, 3.0, 5.2, 2.3, 2], [6.3, 2.5, 5.0, 1.9, 2],
    [6.5, 3.0, 5.2, 2.0, 2], [6.2, 3.4, 5.4, 2.3, 2], [5.9, 3.0, 5.1, 1.8, 2],
];

/**
 * Convert Iris data arrays to `tf.Tensor`s.
 *
 * @param data The Iris input feature data, an `Array` of `Array`s, each element
 *             of which is assumed to be a length-4 `Array` (for petal length, petal
 *             width, sepal length, sepal width).
 * @param targets An `Array` of numbers, with values from the set {0, 1, 2}:
 *                representing the true category of the Iris flower. Assumed to have the same
 *                 array length as `data`.
 * @param testSplit Fraction of the data at the end to split as test data: anumber between 0 and 1.
 * @return A length-4 `Array`, with
 *           - training data as `tf.Tensor` of shape [numTrainExapmles, 4].
 *           - training one-hot labels as a `tf.Tensor` of shape [numTrainExamples, 3]
 *           - test data as `tf.Tensor` of shape [numTestExamples, 4].
 *           - test one-hot labels as a `tf.Tensor` of shape [numTestExamples, 3]
 */
function convertToTensors(data, targets, testSplit) {
    const numExamples = data.length;
    if (numExamples !== targets.length) {
        throw new Error('data and split have different numbers of examples');
    }

    // Randomly shuffle 'data' and 'targets'.
    // 先将预处理后的dataByClass和targetByClass洗牌。
    // 这里的做法是先创建一个相同长度的数组用于存放数组下标，再将改数组洗牌，然后遍历该随机化后的下标
    // 数组，按随机的下标顺序从原始的dataByClass和targetByClass中取出对应的值，就得到了它们随机
    // 化后的值。
    const indices = [];
    for (let i = 0; i < numExamples; ++i) {
        indices.push(i);
    }
    tf.util.shuffle(indices);

    const shuffledData = [];
    const shuffledTargets = [];
    for (let i = 0; i < numExamples; ++i) {
        shuffledData.push(data[indices[i]]);
        shuffledTargets.push(targets[indices[i]]);
    }

    // Split the data into a training set and a tet set, based on 'testSplit'.
    // 计算用于在训练阶段的训练和验证样本数。
    // testSplit在本案例中定义是0.15，就是说训练输入数据集合汇中最后15%的训练样本用做验证，
    const numTestExamples = Math.round(numExamples * testSplit);
    const numTrainExamples = numExamples - numTestExamples;

    const xDims = shuffledData[0].length;

    // Create a 2D 'tf.Tensor' to hold the feature data.
    // 将随机化以后的输入样本数据集合转成二维张量。
    // [numExamples, xDims] => [样本数量，每个样本的特征数量] => [axis 0，axis 1]d
    // 样本数量为第1个秩（即轴axis 0）的长度，每个样本的特征数量为第2个秩（即axis 1）的长度。
    const xs = tf.tensor2d(shuffledData, [numExamples, xDims]);

    // Create a 1D 'tf.Tensor' to hold the labels, and convert the number label
    // from the set {0, 1, 2} into one-hot encoding (.e.g., 0 --> [1, 0, 0]).
    // ***在研究解决分类问题的模型之前，我们需要重点介绍在此多类分类任务中分类目标（物种）的表示方式。将在这里
    // 重点介绍独热编码。
    // 到目前为止，我们在该例子项目集合中看到的所有机器学习示例都涉及简单的目标表示，例如下载时间预测问题和
    // Boston Housing问题中的单个数字，以及0-1的二进制表示钓鱼检测问题中的目标。但是，在本节问题中，以一种
    // 不太熟悉的方式（称为one-hot编码，即独热编码、一位有效编码）来表示这三种鸢尾花。shuffledTargets为随
    // 机化后的鸢尾花类型数组，以[1, 0, 2]为例，先调用把其转为元素为int32的一维张量，实际print出来还是[1, 0, 2]，
    // 再对其做独热编码，独热深度为IRIS_NUM_CLASSES即3，表示最后一个轴的长度为3（放到这里就表示每个子数组的
    // 元素个数为3个），编码的结果是返回一个二维张量：
    // [
    //     [0, 1, 0],
    //     [1, 0, 0],
    //     [0, 0, 1],
    // ]
    // 如果上所示，每一行只有一个数字是1（热、有效），其他都是0（非热，非有效），这就是独热的含义。
    const ys = tf.oneHot(tf.tensor1d(shuffledTargets).toInt(), IRIS_NUM_CLASSES);
    // 尽管上面有内容介绍了独热编码，你也可能对上述tfjs中的独热编码的结果感到不可理解，它其实是这么计算出来的：
    // 首先，tf.tensor1d(shuffledTargets).toInt()的返回张量，在这里被称为indices，即索引集合张量，严
    // 格上，它的元素都是从0开始的整数数字。第二个参数为depth，就是编码后输出张量的最后一个轴（秩）的长度，这
    // 在上面的内容也提到过。就当前案例来讲，在indices为[1, 0, 2]，depth为3的情况下，index 0应该对应100
    // 即[1, 0, 0]编码，其原理就是基于独热编码是使用N位状态寄存器来对N个状态进行编码的：对于indices来说，它
    // 内部最小数为0，最大数为2，即表示它内部有3个状态（即便没有1，比如indices为[0, 0, 2]，也是按最大和最小
    // 数的差异来计算的，也是表示有3个状态的。当然和每个状态的排列先后顺序也无关系），就是说1、0、2这三个数字表
    // 示的索引，对于它们每个来说，我们可以在进行编码前暂时用状态自己来代替，表示每个数字有3位，以1为例，合起来
    // 就是3个1，即111，状态的数量决定了每个状态的位数，也就是整个indices在独热编码输出后，其张量形状就变成了
    // 下面这样：
    // [
    //     [1, 1, 1],    // => 1
    //     [0, 0, 0],    // => 0
    //     [2, 2, 2],    // => 2
    // ]
    // 如上所示，axis 0长度为状态数，axis 1长度为depth，编码后的张量形状相当于状态数*depth，或features*depth。
    // 还有一点需要注意的就是，这里我们的depth也和状态数量一致，恰好也为3。如果depth为4，那在编码后indices的
    // 形状就应该是这样的：
    // [
    //     [1, 1, 1, 0],    // => 1
    //     [0, 0, 0, 0],    // => 0
    //     [2, 2, 2, 0],    // => 2
    // ]
    // 如上所示，因为depth为4，但状态数只有3，所以在最后一个轴上缺失了1位状态，默认就用0来填充。
    // 上面提到的只是depth比状态数量多的情况，那么当depth比状态数量少又怎么办呢？比如depth为2，形状就应该是这样的：
    // [
    //     [1, 1],    // => 1
    //     [0, 0],    // => 0
    //     [2, 2],    // => 2
    // ]
    // 没错，每个状态的位数从末尾开始直接被切掉了1位，只保留了和depth一样的位数。也就是说在后面做编码的时候，被切掉
    // 的这个位就可以不考虑了。
    //
    // 接着我们再深入地对编码过程进行介绍，还是以正常的3个状态和depth为3进行编码，即以第一个矩阵的结构为编码后的输出
    // 的张量形状。首先，我们先把所有索引按大小顺序列出来：0、1、2，这将作为一个对照表和每个状态（3位）的每位进行比较，
    // 然后依次进行编码：
    //
    // 对0进行编码：
    //    首先预设0为000
    //
    //    比较第一位：
    //    000
    //    012
    //    上下按位比较：000的第一位为0，和012中的第一位0一致，那这一位就是有效位，填充为1，此时000变为100。
    //
    //    比较第二位：
    //    100
    //    012
    //    上下按位比较：100的第二位为0，和012中的第二位1不一致，那这一位就是非有效位，填充为0，此时100还是100。
    //
    //    比较第三位：
    //    100
    //    012
    //    上下按位比较：100的第三位为0，和012中的第三位2不一致，那这一位就是非有效位，填充为0，此时100还是100。
    //
    // 对0以012进行三位状态的独热编码到此结束，最后得出编码后的0为100，表示为一维张量形式即[1, 0, 0]。
    //
    // 然后对于1和2的编码我们不再赘述，以上述步骤类推，可知编码后的1为010，编码后的2为001。整体即：
    // 0 => 100 => [1, 0, 0]
    // 1 => 010 => [0, 1, 0]
    // 2 => 001 => [0, 0, 1]
    //
    // 然后我们以此编码映射来替换我们的张量，替换后的张量即为：
    // [
    //     [0, 1, 0],    // => 1
    //     [1, 0, 0],    // => 0
    //     [0, 0, 1],    // => 2
    // ]
    //
    // 至此，indices张量在经过depth为3的oneHot独热编码以后就是上面这个二维的张量，和本段内容靠前区域的介绍结果一致。
    // 总结状态数和depth相同或不同的情况下的编码结果如下：
    // indices为[1, 0, 2]，depth为3，独热编码输出：
    // [
    //     [0, 1, 0],    // => 1
    //     [1, 0, 0],    // => 0
    //     [0, 0, 1],    // => 2
    // ]
    // indices为[1, 0, 2]，depth为4，独热编码输出：
    // [
    //     [0, 1, 0, 0],    // => 1
    //     [1, 0, 0, 0],    // => 0
    //     [0, 0, 1, 0],    // => 2
    // ]
    // indices为[1, 0, 2]，depth为2，独热编码输出：
    // [
    //     [0, 1],    // => 1
    //     [1, 0],    // => 0
    //     [0, 0],    // => 2
    // ]
    //
    // 至此还缺少了对一个特殊情况的解释，就是万一有一个状态不是0或者正整数怎么，比如为负整数，因为虽然tfjs的oneHot函数
    // 只接收0和正整数作为indices的元素，但其实它也能处理负数。比如我们这里的indices在编码前是[1, 0, -1, 2]，那么
    // 它最终被编码以后应该是什么样呢？很简单，就是将-1完全处理为所有位都是非有效位即可，这时indices在编码以后就应该是：
    // [
    //     [0, 1, 0],    // => 1
    //     [1, 0, 0],    // => 0
    //     [0, 0, 0],    // => -1
    //     [0, 0, 1],    // => 2
    // ]
    // 其实它除了能兼容负整数，还能兼容字符串，只不过对字符串的处理是把他们当做0来处理的，比如当indices为['x', 'g', 'a']
    // 的时候，编码后应该是：
    // [
    //     [1, 0, 0],    // => 'x'
    //     [1, 0, 0],    // => 'g'
    //     [1, 0, 0],    // => 'a'
    // ]
    //
    // 以上内容只是针对tfjs的oneHot独热编码来看介绍的，有它自己的特点，而且我们也只介绍了该编码函数的第一个和第二个参数，
    // 就是输入的indices和depth。除此之外，它还支持onValue、offValue和dtype这三个可选参数，它们分别表示有效位、非有
    // 效位和输出张量的数据类型。onValue、offValue不传，分别默认为1和0；dtype不传，默认为int32。在Python版的tf中，
    // 该函数的兄弟one_hot，还支持轴axis的传入，它控制了输出张量的形状：状态数*depth、depth*状态数或depth*batch*状态数。
    //
    // 上面的内容中，我们只介绍了独热编码的实现，那么为什么要用它呢，它有哪些优势是我们期望得到的。对于我们这里的案例[1, 0, 2]，
    // 为什么不就让1表示1，让0表示0，让2表示2呢？非要编个码，改为010之类的？因为独热编码有这些优势：
    // 1. 我们可以创造出公平性。每次出现都携带者团队成员的数量，避免了招摇撞骗，夸大自己的占比，比如2可以说我有N个0和2个1，但2只
    // 表示索引位置，它要这么说的话，显然是无道理的。因此每个成员只能是1，只是用来标记是不是你，无法夸大你的比重。大家的值都是1，
    // 避免了你是1我是2，出现谁大谁小从而干扰计算。这一点的科学性解释如下：
    // 大部分算法是基于向量空间中的度量来进行计算的，为了使非偏序关系的变量取值不具备偏序性，而且到圆点是等距的。使用one-hot编码，
    // 将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的距
    // 离计算更加合理。以本项目案例来讲，我们不能说0（iris-setosa）比2（iris-virginica）更接近1（iris-versicolor），这明
    // 显是不正确的。神经网络以实数为基础，并基于数学运算，例如乘法和加法。因此，它们对数字的大小及其顺序非常敏感。如果将类别编码为
    // 单个数字，则它将成为神经网络必须学习的额外非线性关系。相比之下独热编码类别不涉及任何隐含的排序，因此不会以这种方式来增加神经
    // 网络的学习能力，因为这部分非线性关系是没有意义的，这就能防止神经网络走歪了。
    // 2. 与整数相比，神经网络输出连续的浮点型值要容易得多。对于神经网络的最后一层，更为优雅自然的方法便是输出一些单独的浮点型数字，
    // 通过类似于 sigmoid激活函数，每个浮点数在[0，1]区间内用于二分类。在这种方法中，每个数字都是模型对输入示例属于相应类别概率
    // 的估计，越接近1属于相应类别的概率越高，越接近0责反之。这正是独热编码的目的：这是概率分数的正确答案，模型应针对该分数通过训练
    // 过程进行调整。
    //
    // 但是独热编码也是有局限性的：
    // 1. 首先它不适合大量的数据。如果总共有5条数据，那其中一条是这么表示[1, 0, 0, 0, 0]，如果是10条，这么表示
    // [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]。如果是5000条，那就是1个1，4999个0。这种情况，术语上叫过于稀疏，反而更不利于计算。
    // 2. 另外，也是因为独热很公正公平，所以导致成员间没有个人关系。有时候，尤其当自然语言处理时，我们却希望能表示出每个词语间的相
    // 关性。比如我们在表示心情好坏程度的时候，有如下几种心情：悲伤、郁闷、无聊、微笑、大笑、爆笑。那么，我们假设以0为中心点，负面的
    // 情绪定义为负数，正面的情绪定义为正数。这几种心情可以这么表示：
    // 悲伤    -3
    // 郁闷    -2
    // 无聊    -1
    // 微笑    1
    // 大笑    2
    // 爆笑    3
    // 这样，我们就可以了解他们之间的关系了：爆笑（3）程度要大于微笑（1）。大笑（2）和郁闷（-2）是完全相反的状态。这种带成员关系
    // 的数据，就不合适用独热编码的方式来表示了。
    //
    // ***独热编码适用于一般的分类问题，比如手写数字识别，OCR识别，花朵种类识别（就像本项目案例一样），因为一般的训练集存储的都不
    // 是独热编码。我们这里存储的鸢尾花类型是数字0、1、2，我就就可以先对其进行独热编码后，再用于训练中的计算。

    // Split the data into training and test sets, using `slice`.
    const xTrain = xs.slice([0, 0], [numTrainExamples, xDims]);
    const xTest = xs.slice([numTrainExamples, 0], [numTestExamples, xDims]);

    const yTrain = ys.slice([0, 0], [numTrainExamples, IRIS_NUM_CLASSES]);
    const yTest = ys.slice([0, 0], [numTestExamples, IRIS_NUM_CLASSES]);

    return [xTrain, yTrain, xTest, yTest];
}

/**
 * Obtains Iris data, split into training and test sets.
 *
 * @param testSplit Fraction of the data at the end to split as test data: a
 *                  number between 0 and 1.
 *
 * @param return A length-4 'Array', with
 *               - training data as an 'Array' of length-4 'Array' of numbers.
 *               - training labels as an 'Array' of numbers, with the same length as the return training
 *                 data above. Each element of the 'Array' is from the set {0, 1, 2}.
 *               - test data as an 'Array' of length-4 'Array' of numbers.
 *               - test labels as an 'Array' of numbers, with the same length as the return test data above.
 *                 Each element of the 'Array' is from the set {0, 1, 2}.
 */
function getIrisData(testSplit) {
    return tf.tidy(() => {
        // 对原始数据集进行预处理，整理出用于训练的带特征的输入数据集合和对应的标签集合，
        const dataByClass = [];
        const targetsByClass = [];
        for (let i = 0; i < IRIS_CLASSES.length; ++i) {
            dataByClass.push([]);      // 带特征的输入数据集合。
            targetsByClass.push([]);   // 标签集合。
        }
        for (const example of IRIS_DATA) {
            const target = example[example.length - 1];
            const data = example.slice(0, example.length - 1);
            dataByClass[target].push(data);
            targetsByClass[target].push(target);
        }
        // 预处理后的结构：
        // dataByClass => [
        //     [                               // trainData 1
        //         [5.1, 3.5, 1.4, 0.2, 0],
        //         [4.9, 3.0, 1.4, 0.2, 0],
        //         [4.7, 3.2, 1.3, 0.2, 0],
        //         ...
        //     ],
        //     [                               // trainData 2
        //         [7.0, 3.2, 4.7, 1.4, 1],
        //         [6.4, 3.2, 4.5, 1.5, 1],
        //         [6.9, 3.1, 4.9, 1.5, 1],
        //         ...
        //     ],
        //     [                               // trainData 3
        //         [6.3, 3.3, 6.0, 2.5, 2],
        //         [5.8, 2.7, 5.1, 1.9, 2],
        //         [7.1, 3.0, 5.9, 2.1, 2],
        //         ...
        //     ]
        // ]
        //
        // targetsByClass => [
        //     [0, 0, 0, ...],     // testData 1
        //     [1, 1, 1, ...],     // testData 2
        //     [2, 2, 2, ...],     // testData 3
        // ]
        //
        // 这里为什么是3套数据呢，其实就是我们这里要分别针对鸢尾花的3个类型进行训练，在下面的for循环中，
        // 我们就要把这3套数据的张量给构建出来，如上所示，trainData 1将和testData 1搭配，其他以此类推。

        const xTrains = [];
        const yTrains = [];
        const xTests = [];
        const yTests = [];
        for (let i = 0; i < IRIS_CLASSES.length; ++i) {
            const [xTrain, yTrain, xTest, yTest] = convertToTensors(dataByClass[i], targetsByClass[i], testSplit);
            xTrains.push(xTrain);
            yTrains.push(yTrain);
            xTests.push(xTest);
            yTests.push(yTest);
        }

        const concatAxis = 0;

        return [
            tf.concat(xTrains, concatAxis), tf.concat(yTrains, concatAxis),
            tf.concat(xTests, concatAxis), tf.concat(yTests, concatAxis)
        ];
    });
}